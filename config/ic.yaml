#################################################################################
## ALGORITHM
##################################################################################
# Agent types (algorithms) to play with
agent_types: ["further", "further"]

# Learning rate for actor
actor_lr: 0.0001

# Learning rate for critic
critic_lr: 0.0005

# Learning rate for gain
gain_lr: 0.02

# Learning rate for inference
inference_lr: 0.0005

# Number of neurons for hidden network
n_hidden: 32

# Number of latent variables
n_latent: 5

# Weight for entropy
entropy_weight: 0.3

# Max timestep to train agents
max_timestep: 10000


#################################################################################
## REPLAY BUFFER
##################################################################################
# Sampling method for training actor and critic [random, all, recent]
sampling_mode: "recent"

# Number of samples to use for each train iteration
batch_size: 64


#################################################################################
## ENV
##################################################################################
# OpenAI gym environment name
env_name: "IC-v0"

# Episode is terminated when max timestep is reached
ep_horizon: 150
